# Web Scraping System Documentation
**CloudClearingAPI - October 19, 2025**

## Overview

The web scraping system collects real-time land price data from Indonesian real estate portals with a sophisticated cascading fallback architecture:

**Priority 1:** Live scraping from Lamudi.co.id (primary source)  
**Priority 2:** Live scraping from Rumah.com (secondary source)  
**Priority 3:** Live scraping from 99.co (tertiary source) ‚ú® **NEW in Phase 2A.5**  
**Priority 4:** Cached results from any source (if < 24-48 hours old)  
**Priority 5:** Static regional benchmarks (last resort)

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  LandPriceOrchestrator                      ‚îÇ
‚îÇ  Coordinates multiple scrapers with 3-tier fallback logic   ‚îÇ
‚îÇ  Phase 2A.5: Lamudi ‚Üí Rumah.com ‚Üí 99.co                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ               ‚îÇ               ‚îÇ                ‚îÇ
         ‚ñº               ‚ñº               ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Lamudi    ‚îÇ  ‚îÇ  Rumah.com  ‚îÇ  ‚îÇ    99.co    ‚îÇ  ‚îÇ  Benchmark  ‚îÇ
‚îÇ  Scraper    ‚îÇ  ‚îÇ   Scraper   ‚îÇ  ‚îÇ  Scraper    ‚îÇ  ‚îÇ   Fallback  ‚îÇ
‚îÇ (Priority 1)‚îÇ  ‚îÇ (Priority 2)‚îÇ  ‚îÇ (Priority 3)‚îÇ  ‚îÇ (Priority 5)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ               ‚îÇ               ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ                                
         ‚ñº                                
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Cache Storage (Priority 4)               ‚îÇ
‚îÇ  JSON files with 24-48h expiry (output/scraper_cache/)     ‚îÇ
‚îÇ  Checks: lamudi ‚Üí rumah_com ‚Üí 99.co cached files           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Components

### 1. Base Scraper (`base_scraper.py`)

**Purpose:** Provides common functionality for all scrapers

**Key Features:**
- ‚úÖ **Cache-first architecture** - Always checks cache before scraping
- ‚úÖ **User-agent rotation** - 5 different agents to avoid detection
- ‚úÖ **Rate limiting** - 2 second minimum between requests
- ‚úÖ **Request timeout** - 15s primary, 30s fallback for retries
- ‚úÖ **Retry logic** - Up to 3 attempts with exponential backoff ‚ú® **Phase 2A.6**
- ‚úÖ **Smart retry strategy** - Retries server errors (5xx), skips client errors (4xx) ‚ú® **Phase 2A.6**
- ‚úÖ **Exponential backoff** - 1s ‚Üí 2s ‚Üí 4s ‚Üí ... up to 30s max ‚ú® **Phase 2A.6**
- ‚úÖ **Error handling** - Graceful degradation on failures

**Classes:**
- `ScrapedListing` - Single listing data structure
- `ScrapeResult` - Complete scrape operation result
- `BaseLandPriceScraper` - Abstract base class

**Example Usage:**
```python
from src.scrapers.base_scraper import BaseLandPriceScraper

# Subclass must implement:
# - _scrape_live(region_name, max_listings) -> ScrapeResult
# - get_source_name() -> str

class MyScraper(BaseLandPriceScraper):
    def get_source_name(self) -> str:
        return "my_site"
    
    def _scrape_live(self, region_name, max_listings):
        # Implement scraping logic
        pass
```

### 2. Lamudi Scraper (`lamudi_scraper.py`)

**Purpose:** Scrape land listings from Lamudi.co.id

**URL Structure:**
```
https://www.lamudi.co.id/tanah/buy/{location}/?sort=newest
```

**Parsing Strategy:**
- Uses flexible CSS selectors with regex patterns
- Handles Indonesian price formats (Miliar, Juta, Ribu)
- Extracts: price, size, location, listing date, URL
- Calculates price per m¬≤ automatically

**Price Parsing Examples:**
```
"Rp 1.500.000.000" ‚Üí 1,500,000,000 IDR
"Rp 1,5 Miliar"    ‚Üí 1,500,000,000 IDR
"Rp 500 Juta"      ‚Üí 500,000,000 IDR
"1.5M"             ‚Üí 1,500,000 IDR
```

**Size Parsing Examples:**
```
"1.000 m¬≤"  ‚Üí 1,000 m¬≤
"500 m2"    ‚Üí 500 m¬≤
"1,5 ha"    ‚Üí 15,000 m¬≤
```

### 3. Rumah.com Scraper (`rumah_scraper.py`)

**Purpose:** Scrape land listings from Rumah.com

**URL Structure:**
```
https://www.rumah.com/properti/tanah/{location}?sort=terbaru
```

**Parsing Strategy:**
- Similar to Lamudi with site-specific selectors
- Handles "LT:" (Luas Tanah) prefixes
- Supports data attributes for size extraction
- Same Indonesian format handling

### 4. 99.co Scraper (`ninety_nine_scraper.py`) ‚ú® **NEW in Phase 2A.5**

**Purpose:** Third-tier fallback scraper for 99.co real estate portal

**URL Structure:**
```
https://www.99.co/id/jual/tanah/{province}
```

**Key Differences from Lamudi/Rumah.com:**
- **Province-based URLs** - Requires mapping regions to provinces
- **Indonesian price formats** - Handles Miliar/Juta/Ribu variations
- **Flexible selectors** - Multiple fallback patterns for HTML structure
- **Size extraction** - Supports m¬≤/m2/sqm/hectare formats

**Province Mapping:**
```python
region_mapping = {
    'yogyakarta': 'yogyakarta',
    'sleman': 'yogyakarta',
    'bantul': 'yogyakarta',
    'jakarta': 'jakarta',
    'bandung': 'bandung',
    'surabaya': 'surabaya',
    'semarang': 'jawa-tengah',
    'bali': 'bali'
}
```

**Price Parsing (99.co specific):**
```python
# Examples from 99.co listings:
"Rp 2 Miliar"      ‚Üí 2,000,000,000 IDR
"Rp 500 Juta"      ‚Üí 500,000,000 IDR
"Rp 50 Ribu/m¬≤"    ‚Üí 50,000 IDR/m¬≤
"2.5 M"            ‚Üí 2,500,000 IDR
```

**Integration Pattern:**
```python
from src.scrapers.ninety_nine_scraper import NinetyNineScraper

scraper = NinetyNineScraper(
    cache_dir="output/scraper_cache",
    cache_expiry_hours=24
)

result = scraper.scrape("Sleman Yogyakarta", max_listings=20)
# Returns: ScrapeResult with data_source='99.co'
```

### 5. Scraper Orchestrator (`scraper_orchestrator.py`)

**Purpose:** Coordinate multiple scrapers with intelligent 3-tier fallback (Phase 2A.5)

**Cascading Logic:**

```python
def get_land_price(region_name):
    # Phase 1: Live Scraping (3-tier cascading fallback)
    if live_scraping_enabled:
        # Priority 1: Try Lamudi first
        result = lamudi.scrape(region_name)
        if result.success:
            return result
        
        # Priority 2: Try Rumah.com second
        result = rumah_com.scrape(region_name)
        if result.success:
            return result
        
        # Priority 3: Try 99.co third (Phase 2A.5)
        result = ninety_nine.scrape(region_name)
        if result.success:
            return result
        
        logger.warning("All 3 live scraping sources failed")
    
    # Phase 2: Check Cache from all 3 sources
    # Tries: lamudi ‚Üí rumah_com ‚Üí 99.co cached files
    cache = check_cache(region_name)
    if cache:
        return cache
    
    # Phase 3: Static Benchmark (guaranteed fallback)
    return get_benchmark_fallback(region_name)
```

**Data Source Tracking (Phase 2A.5):**

All results include `data_source` field for transparency:
- **Live sources:** `'lamudi'`, `'rumah.com'`, `'99.co'`
- **Cached sources:** `'lamudi_cached'`, `'rumah_com_cached'`, `'99.co_cached'`
- **Fallback:** `'static_benchmark'`

**Status Reporting:**
```python
status = orchestrator.get_orchestrator_status()
# Returns:
{
    'total_sources': 3,
    'scrapers': [
        {'name': 'Lamudi', 'slug': 'lamudi', 'priority': 1, ...},
        {'name': 'Rumah.com', 'slug': 'rumah_com', 'priority': 2, ...},
        {'name': '99.co', 'slug': '99.co', 'priority': 3, ...}
    ],
    'cache_directory': 'output/scraper_cache',
    'live_scraping_enabled': True
}
```

**Regional Benchmarks:**

| Region      | Price (IDR/m¬≤) | Annual Appreciation | Liquidity |
|-------------|----------------|---------------------|-----------|
| Jakarta     | 8,500,000      | 15%                 | High      |
| Bali        | 12,000,000     | 20%                 | High      |
| Yogyakarta  | 4,500,000      | 12%                 | Moderate  |
| Surabaya    | 6,500,000      | 14%                 | High      |
| Bandung     | 5,000,000      | 13%                 | Moderate  |
| Semarang    | 3,500,000      | 11%                 | Moderate  |

## Integration with Financial Metrics

### Modified `_estimate_current_land_value()` Flow

**Before (Static Only):**
```python
def _estimate_current_land_value(region_name):
    benchmark = find_nearest_benchmark(region_name)
    base_value = benchmark['current_avg']
    return base_value * adjustments
```

**After (Live ‚Üí Cache ‚Üí Static):**
```python
def _estimate_current_land_value(region_name):
    # Try live scraping / cache
    if orchestrator:
        price_data = orchestrator.get_land_price(region_name)
        if price_data['success']:
            base_value = price_data['average_price_per_m2']
            source = price_data['data_source']  # 'lamudi', 'rumah_com', etc.
            return base_value * adjustments
    
    # Fallback to static
    benchmark = find_nearest_benchmark(region_name)
    base_value = benchmark['current_avg']
    return base_value * adjustments
```

## Configuration

### `config.yaml` Settings

```yaml
web_scraping:
  enabled: true                 # Enable/disable live scraping
  cache_expiry_hours: 24        # Cache validity (24-48h recommended)
  max_listings_per_site: 20     # Limit per scrape
  
  # Request timeout settings (Phase 2A.6)
  request_timeout: 15           # Primary timeout (seconds)
  fallback_timeout: 30          # Fallback timeout for retries (seconds)
  
  # Retry logic configuration (Phase 2A.6)
  max_retries: 3                # Maximum retry attempts per request
  initial_backoff: 1            # Initial backoff delay (seconds)
  max_backoff: 30               # Maximum backoff delay (seconds)
  backoff_multiplier: 2         # Exponential multiplier (2 = double each time)
  
  # Rate limiting
  rate_limit_seconds: 2         # Minimum delay between requests
  
  sites:
    lamudi:
      enabled: true
      base_url: "https://www.lamudi.co.id"
    rumah_com:
      enabled: true
      base_url: "https://www.rumah.com"
    ninety_nine:  # Phase 2A.5
      enabled: true
      base_url: "https://www.99.co"
```

### Environment Variables

None required - scrapers work without authentication.

## Usage Examples

### Basic Usage

```python
from src.scrapers.scraper_orchestrator import LandPriceOrchestrator

# Initialize orchestrator (Phase 2A.6: with retry config)
config = {
    'max_retries': 3,
    'initial_backoff': 1,
    'max_backoff': 30,
    'backoff_multiplier': 2,
    'request_timeout': 15,
    'fallback_timeout': 30
}

orchestrator = LandPriceOrchestrator(
    cache_expiry_hours=24,
    enable_live_scraping=True,
    config=config  # Phase 2A.6: Pass retry config
)

# Get land price data (with automatic retry on failures)
result = orchestrator.get_land_price("Sleman Yogyakarta", max_listings=20)

print(f"Average: Rp {result['average_price_per_m2']:,.0f}/m¬≤")
print(f"Source: {result['data_source']}")  # Phase 2A.5: 'lamudi', 'rumah.com', '99.co', 
                                            # 'lamudi_cached', 'rumah_com_cached', '99.co_cached',
                                            # or 'static_benchmark'
print(f"Listings: {result['listing_count']}")
```

### Financial Metrics Integration

```python
from src.core.financial_metrics import FinancialMetricsEngine

# Initialize with web scraping enabled
engine = FinancialMetricsEngine(
    enable_web_scraping=True,
    cache_expiry_hours=24
)

# Calculate projection (will use live prices if available)
projection = engine.calculate_financial_projection(
    region_name="Sleman North",
    satellite_data=satellite_data,
    infrastructure_data=infrastructure_data,
    market_data=market_data,
    scoring_result=scoring_result
)

# Data sources used are listed in projection
print(projection.data_sources)
# Example: ['satellite_sentinel2', 'lamudi', 'openstreetmap']
```

### Cache Management

```python
# Check cache status
from pathlib import Path
cache_dir = Path("output/scraper_cache")
cache_files = list(cache_dir.glob("*.json"))
print(f"Cached regions: {len(cache_files)}")

# Clear cache for specific region from all sources (Phase 2A.5)
orchestrator.lamudi.clear_cache("Sleman Yogyakarta")
orchestrator.rumah_com.clear_cache("Sleman Yogyakarta")
orchestrator.ninety_nine.clear_cache("Sleman Yogyakarta")

# Clear all cache from all sources
orchestrator.lamudi.clear_cache()
orchestrator.rumah_com.clear_cache()
orchestrator.ninety_nine.clear_cache()
```

## Cache Structure

**Location:** `output/scraper_cache/`

**File Format:** `{source}_{region}.json`

**Example:** `lamudi_sleman_yogyakarta.json`, `rumah_com_sleman_yogyakarta.json`, `99.co_sleman_yogyakarta.json`

**Content Structure:**
```json
{
  "region_name": "Sleman Yogyakarta",
  "average_price_per_m2": 5250000,
  "median_price_per_m2": 4800000,
  "listing_count": 15,
  "listings": [
    {
      "price_per_m2": 5500000,
      "total_price": 825000000,
      "size_m2": 150,
      "location": "Sleman, Yogyakarta",
      "listing_date": "2 hari lalu",
      "source_url": "https://www.lamudi.co.id/...",
      "listing_type": "land"
    }
  ],
  "source": "lamudi",  // Phase 2A.5: Can be "lamudi", "rumah_com", or "99.co"
  "scraped_at": "2025-10-19T14:30:00",
  "success": true,
  "error_message": null
}
```

## Error Handling

### Common Issues & Solutions

**1. No listings found from all sources**
```
Symptom: listing_count = 0 from Lamudi, Rumah.com, and 99.co; data_source = 'static_benchmark'
Cause: Site layouts changed, region name doesn't match any province, or temporary site issues
Solution: System automatically falls back to benchmark (Phase 2A.5 guarantees data availability)
```

**2. Rate limiting / IP blocking**
```
Symptom: Request timeouts or HTTP 429 errors
Cause: Too many requests
Solution (Phase 2A.6 enhanced): 
  - Automatic retry with exponential backoff (1s ‚Üí 2s ‚Üí 4s ‚Üí ...)
  - Increase rate_limit_seconds in config (default: 2s)
  - Increase max_backoff for slower retry progression
  - Use cache (already scraped data)
  - Wait 15-30 minutes before retrying if IP blocked
```

**3. Parsing failures from specific source**
```
Symptom: Lamudi or Rumah.com listings found but price_per_m2 = 0
Cause: Site HTML structure changed for that specific portal
Solution:
  - System automatically tries next source (Rumah.com ‚Üí 99.co ‚Üí cache ‚Üí benchmark)
  - Check logs for parsing errors from specific scraper
  - Update CSS selectors in affected scraper only
  - Other sources continue working (Phase 2A.5 resilience)
```

**4. Cache stale**
```
Symptom: data_source contains '_cached', cache_age_hours > 48
Cause: Live scraping failed, using old cache
Solution:
  - Normal operation during fallback
  - Will retry live scraping on next request
  - Consider clearing cache to force benchmark use
```

## Testing

### Run Test Suite

```bash
# Test multi-source fallback (Phase 2A.5)
python3 test_multi_source_fallback.py

# Test request hardening (Phase 2A.6)
python3 test_request_hardening.py

# Test individual scrapers
python3 test_web_scraping.py
```

**Test Coverage:**
1. ‚úÖ 3-tier cascading fallback (Lamudi ‚Üí Rumah.com ‚Üí 99.co)
2. ‚úÖ Data source tracking transparency
3. ‚úÖ Priority order validation
4. ‚úÖ Retry logic with exponential backoff (Phase 2A.6)
5. ‚úÖ Timeout handling (primary 15s, fallback 30s) (Phase 2A.6)
6. ‚úÖ Smart retry strategy (5xx yes, 4xx no) (Phase 2A.6)
7. ‚úÖ Config propagation to all scrapers (Phase 2A.6)
8. ‚úÖ Cache persistence and expiry (all 3 sources)
9. ‚úÖ Fallback to static benchmark
10. ‚úÖ Financial metrics integration
11. ‚úÖ Error handling resilience

### Expected Output (Phase 2A.5 Tests)

```
================================================================================
MULTI-SOURCE SCRAPING FALLBACK TEST SUITE - Phase 2A.5
================================================================================

TEST 1: Orchestrator Status (3 Scrapers)
‚úÖ TEST PASSED: All 3 scrapers registered with correct priorities

TEST 2: Fallback to Benchmark (Scraping Disabled)
‚úÖ TEST PASSED: Benchmark fallback works correctly

TEST 3: Data Source Tracking
‚úÖ TEST PASSED: Data source tracking works correctly

TEST 4: Scraper Priority Order Validation
‚úÖ TEST PASSED: Priority order is correct (Lamudi ‚Üí Rumah.com ‚Üí 99.co)

================================================================================
TEST SUMMARY
Tests Passed: 4/4
üéâ ALL TESTS PASSED - Multi-source fallback working correctly!
================================================================================
```

### Expected Output (Phase 2A.6 Tests)

```
================================================================================
REQUEST HARDENING TEST SUITE - Phase 2A.6
================================================================================

TEST 1: Retry Configuration
‚úÖ TEST PASSED: Custom retry configuration applied correctly

TEST 2: Default Configuration
‚úÖ TEST PASSED: Default configuration working correctly

TEST 3: Retry on Timeout
‚è≥ Retrying in 1.2s (backoff for timeout)...
‚úÖ TEST PASSED: Retry logic working correctly on timeouts

TEST 4: Max Retries Exhausted
‚úÖ TEST PASSED: Max retries limit enforced correctly

TEST 5: No Retry on Client Errors (4xx)
‚úÖ TEST PASSED: No retry on client errors (correct behavior)

TEST 6: Retry on Server Errors (5xx)
‚úÖ TEST PASSED: Retry on server errors working correctly

TEST 7: Exponential Backoff Timing
  Retry 1: 1s backoff (2^0 = 1x initial)
  Retry 2: 2s backoff (2^1 = 2x initial)
  Retry 3: 4s backoff (2^2 = 4x initial)
‚úÖ TEST PASSED: Exponential backoff calculation correct

TEST 8: Config Propagation to Scrapers
‚úÖ TEST PASSED: Config properly propagated to all scrapers

================================================================================
TEST SUMMARY
Tests Passed: 8/8
üéâ ALL TESTS PASSED - Request hardening working correctly!
================================================================================
```

## Performance Characteristics

| Operation              | Time    | Notes                                      |
|------------------------|---------|---------------------------------------------|
| Live scraping          | 5-15s   | Depends on site response time               |
| Live scraping (retry)  | 10-45s  | With exponential backoff (Phase 2A.6)       |
| Cache retrieval        | <0.1s   | Instant from disk                           |
| Benchmark fallback     | <0.01s  | In-memory lookup                            |
| Full orchestration     | 5-15s   | First request (with scraping)               |
| Subsequent requests    | <0.1s   | Uses cache                                  |

**Retry Timing (Phase 2A.6):**
- **Attempt 1:** Immediate (0s wait)
- **Attempt 2:** 1s backoff + jitter (¬±20%)
- **Attempt 3:** 2s backoff + jitter
- **Attempt 4:** 4s backoff + jitter (if max_retries = 4)
- **Max backoff:** 30s (configurable)

## Data Quality

### Confidence Levels

| Data Source           | Confidence | Notes                              |
|-----------------------|------------|------------------------------------|
| Live scraping         | 85%        | Real listings, current prices      |
| Cached (< 24h)        | 85%        | Recent real data                   |
| Cached (24-48h)       | 75%        | Slightly stale                     |
| Static benchmark      | 50%        | Historical averages only           |

### Listing Validation

Scrapers automatically filter out invalid listings:
- ‚úÖ Price > 0
- ‚úÖ Size > 0  
- ‚úÖ Price per m¬≤ within reasonable range (100k - 100M IDR)
- ‚úÖ Complete location information

## Maintenance

### Adding New Scraper (Phase 2A.5 Pattern)

1. Create new scraper class inheriting from `BaseLandPriceScraper`
2. Implement `_scrape_live()` and `get_source_name()`
3. Add to orchestrator in `__init__()` method (e.g., `self.new_scraper = NewScraper(...)`)
4. Update orchestrator's `get_land_price()` cascading logic
5. Update orchestrator's `_check_cache()` method
6. Update orchestrator's `get_orchestrator_status()` method
7. Add site config to `config.yaml`
8. Create tests in `test_multi_source_fallback.py`

**Example:**
```python
# src/scrapers/newsite_scraper.py
from .base_scraper import BaseLandPriceScraper, ScrapeResult

class NewSiteScraper(BaseLandPriceScraper):
    def get_source_name(self) -> str:
        return "newsite"
    
    def _scrape_live(self, region_name, max_listings):
        # Implementation
        pass

# In scraper_orchestrator.py __init__:
self.new_scraper = NewSiteScraper(cache_dir, cache_expiry_hours)

# In get_land_price() method (add after existing sources):
new_scraper_result = self._try_live_scrape(self.new_scraper, region_name, max_listings)
if new_scraper_result['success']:
    return new_scraper_result

# In _check_cache() method:
cache_path = self.new_scraper.cache_dir / f"newsite_{region_name}.json"
if cache_path.exists():
    # Check and load cache...
```

### Updating Selectors

If site layout changes, update CSS selectors in respective scraper:

```python
# Old selector
listing_cards = soup.find_all('div', class_='listing-card')

# Update to new selector
listing_cards = soup.find_all('article', class_='property-item')
```

## Troubleshooting

### Enable Debug Logging

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

Debug output shows:
- URLs requested
- Listings found per page
- Parsing successes/failures
- Cache hits/misses
- Fallback triggers

### Check Cache Files

```bash
# View cache directory
ls -lh output/scraper_cache/

# View cache content
cat output/scraper_cache/lamudi_sleman_yogyakarta.json | jq
```

### Test Individual Scrapers

```bash
# Test Lamudi scraper
python3 src/scrapers/lamudi_scraper.py

# Test Rumah.com scraper
python3 src/scrapers/rumah_scraper.py

# Test 99.co scraper (Phase 2A.5)
python3 src/scrapers/ninety_nine_scraper.py

# Test orchestrator (all 3 sources)
python3 src/scrapers/scraper_orchestrator.py

# Test multi-source fallback
python3 test_multi_source_fallback.py
```

## Future Enhancements

### Planned Features
- [ ] Proxy rotation for higher request volumes
- [ ] JavaScript rendering for dynamic sites (Selenium/Playwright)
- [ ] Historical price tracking database
- [ ] Price trend analysis from cached data
- [ ] Multi-city batch scraping
- [ ] Automated selector updates via ML

### Integration Opportunities
- [ ] DEM terrain data for development cost accuracy
- [ ] Government land registry data (BPN)
- [ ] Zoning and land use regulations
- [ ] Infrastructure development plans

## Dependencies

```bash
pip install requests beautifulsoup4 lxml
```

- **requests** - HTTP client
- **beautifulsoup4** - HTML parsing
- **lxml** - Fast XML/HTML parser

## License & Legal

**Important:** Web scraping must comply with:
1. ‚úÖ `robots.txt` of target sites
2. ‚úÖ Terms of Service
3. ‚úÖ Reasonable rate limiting (2s minimum)
4. ‚úÖ No circumvention of authentication

This system is designed for **personal research and investment analysis** only. Commercial use may require permission from site owners.

## Support

For issues or questions:
1. Check logs with DEBUG logging enabled
2. Review test suite output
3. Verify cache contents
4. Test with known-good region (e.g., "Sleman Yogyakarta")

## Changelog

**October 25, 2025 - Phase 2A.6** - Request hardening with retry logic
- ‚úÖ Implemented retry logic with exponential backoff (up to 3 attempts)
- ‚úÖ Configurable timeout handling (15s primary, 30s fallback for retries)
- ‚úÖ Smart retry strategy: retries server errors (5xx), skips client errors (4xx)
- ‚úÖ Exponential backoff: 1s ‚Üí 2s ‚Üí 4s ‚Üí ... up to 30s max
- ‚úÖ Added jitter to backoff (¬±20%) to prevent thundering herd
- ‚úÖ Config-driven retry settings (max_retries, initial_backoff, max_backoff, backoff_multiplier)
- ‚úÖ Enhanced logging for retry attempts and backoff timing
- ‚úÖ Config propagation to all scrapers via orchestrator
- ‚úÖ Comprehensive test suite (test_request_hardening.py) with 8 tests

**October 19, 2025 - Phase 2A.5** - Multi-source scraping fallback
- ‚úÖ Added 99.co scraper as third-tier fallback source
- ‚úÖ Implemented 3-tier cascading: Lamudi ‚Üí Rumah.com ‚Üí 99.co
- ‚úÖ Enhanced cache checking across all 3 sources
- ‚úÖ Added data source tracking transparency (`data_source` field)
- ‚úÖ Created comprehensive test suite (test_multi_source_fallback.py)
- ‚úÖ Updated orchestrator status reporting with priorities
- ‚úÖ Province-based URL mapping for 99.co
- ‚úÖ Indonesian price format parsing (Miliar/Juta/Ribu)

**October 19, 2025** - Initial implementation
- ‚úÖ Base scraper infrastructure
- ‚úÖ Lamudi and Rumah.com scrapers
- ‚úÖ Orchestrator with cascading fallback
- ‚úÖ Cache persistence (24h expiry)
- ‚úÖ Financial metrics integration
- ‚úÖ Comprehensive test suite
